"""LLM client for generating v3.0 enhanced reports with clusters and timeline."""

from datetime import datetime
from typing import Any

from dotenv import load_dotenv
from google import genai
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
)

from src.utils.logger import setup_logger

logger = setup_logger("generator_v3")
load_dotenv()


class LLMClientV3:
    """v3.0 Generator with clusters and timeline support."""

    SYSTEM_PROMPT_V3 = """You are Cloud927 v3.0, an AI Intelligence Analyst.

## v3.0 Enhanced Output Format

```markdown
# ğŸ¤– {date} å…¨çƒ AI & ç§‘æŠ€æ—¥æŠ¥

> **æ ¸å¿ƒå‘ç°ä¸€å¥è¯** - One sentence capturing the most critical signal across all sources

---

## ğŸŒ å…¨çƒè¦é—» (Global News)
| æ—¶é—´ | æ¥æº | æ ‡é¢˜ | é‡è¦æ€§ |
|------|------|------|--------|
| 10:30 | Reuters | [æ ‡é¢˜](url) | â­â­â­ |

## ğŸš€ AI ç§‘æŠ€å‰æ²¿
### ğŸ“Š ä¿¡å·å¼ºåº¦æ¦‚è§ˆ
| é¢†åŸŸ | çƒ­åº¦ | é‡è¦å‘ç° |
|------|------|----------|
| AIæ¨¡å‹ | ğŸ”¥ğŸ”¥ğŸ”¥ | xxx |
| å¼€æºå·¥å…· | ğŸ”¥ğŸ”¥ | xxx |

### ğŸ† é‡ç‚¹èšç„¦ (æ¥è‡ªèšç±»åˆ†æ)
**AIæ¨¡å‹æ›´æ–°**
- [æ ‡é¢˜](url) - å…³é”®æ´å¯Ÿ

## ğŸ‡¨ğŸ‡³ ä¸­å›½åŠ¨æ€
### ğŸ¤– AI å‰æ²¿
- [æ ‡é¢˜](url) - æ´å¯Ÿ

### ğŸ“° ç¤¾ä¼šé‡å¤§
- [æ ‡é¢˜](url) - é‡è¦æ€§è¯„çº§

## ğŸ”® è¶‹åŠ¿è¿½è¸ª
- **æŒç»­å…³æ³¨**: [å®ä½“] - çŠ¶æ€æ›´æ–°
- **æ–°å‡ºç°**: [å®ä½“] - é¦–æ¬¡æŠ¥é“

## ğŸ¯ Cloud927 æ´å¯Ÿ
> **è¶‹åŠ¿åˆ¤æ–­**: [100+ words on trajectory and implications]
> **è¡ŒåŠ¨å»ºè®®**:
> - [ç«‹å³å¯åš]
> - [çŸ­æœŸè§„åˆ’]
> - [é•¿æœŸè§‚å¯Ÿ]

---
*Generated by Cloud927 v3.0 | Sources: 18 data sources | Clusters: {cluster_count}*
```

## Critical Rules
1. **Cross-Source Synthesis** - Use deduplicated and clustered data
2. **Timeline Awareness** - Track entity developments over time
3. **Chinese Output** - Simplified Chinese throughout
4. **Evidence-Based** - Link claims to specific sources
5. **Actionable Insights** - Focus on practical implications
"""

    def __init__(self, api_key: str | None = None):
        self.api_key = api_key
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY is required")

    @property
    def client(self):
        if not hasattr(self, "_client"):
            self._client = genai.Client(api_key=self.api_key)
        return self._client

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=60),
        retry=retry_if_exception_type(Exception),
        before_sleep=before_sleep_log(logger, logger.level),
    )
    def generate_report(
        self,
        raw_results: dict[str, Any],
        clusters: dict[str, list[dict]],
        timeline: dict[str, Any],
        date: datetime | None = None,
    ) -> str:
        """Generate v3.0 enhanced report."""
        if date is None:
            date = datetime.now()

        date_str = date.strftime("%Y-%m-%d")
        cluster_count = len(clusters)

        user_prompt = self._build_v3_prompt(
            raw_results=raw_results,
            clusters=clusters,
            timeline=timeline,
            date_str=date_str,
            cluster_count=cluster_count,
        )

        logger.info(f"Generating v3.0 report for {date_str}")

        try:
            response = self.client.models.generate_content(
                model="gemini-2.0-flash",
                contents=[user_prompt],
                config={"system_instruction": self.SYSTEM_PROMPT_V3},
            )
            report = response.text.strip()
            logger.info(f"v3.0 Report generated: {len(report)} chars")
            return report

        except Exception as e:
            logger.error(f"Failed to generate v3.0 report: {e}")
            raise

    def _format_cluster(self, category: str, items: list[dict]) -> str:
        """Format cluster items."""
        lines = [f"### {category}\n"]

        for i, item in enumerate(items[:5], 1):
            title = item.get("title", "N/A")
            url = item.get("url", "")
            source = item.get("source", "")
            timestamp = item.get("timestamp", 0)
            score = item.get("score", 0) or item.get("importance_score", 0)

            # Format time
            import time as time_module
            if isinstance(timestamp, (int, float)):
                time_str = datetime.fromtimestamp(timestamp).strftime("%H:%M")
            else:
                time_str = timestamp

            lines.append(f"**{i}. [{title}]({url})**")
            lines.append(f"   - æ¥æº: {source} | æ—¶é—´: {time_str} | åˆ†æ•°: {score}")
            lines.append("")

        return "\n".join(lines)

    def _format_timeline(self, timeline: dict[str, Any]) -> str:
        """Format timeline updates."""
        lines = ["## ğŸ”® è¶‹åŠ¿è¿½è¸ª\n"]

        new_entities = timeline.get("new", [])
        updated_entities = timeline.get("updated", [])

        if new_entities:
            lines.append("**ğŸ†• æ–°å‡ºç°çš„å®ä½“:**\n")
            for item in new_entities[:5]:
                entity = item.get("entity", "Unknown")
                event = item.get("first_event", {})
                title = event.get("title", "")[:60]
                lines.append(f"- **{entity}**: {title}...")

        if updated_entities:
            lines.append("\n**ğŸ“ˆ æœ‰æ›´æ–°çš„å®ä½“:**\n")
            for item in updated_entities[:5]:
                entity = item.get("entity", "Unknown")
                new_event = item.get("new_event", {})
                title = new_event.get("title", "")[:60]
                lines.append(f"- **{entity}**: {title}...")

        return "\n".join(lines)

    def _build_v3_prompt(
        self,
        raw_results: dict[str, Any],
        clusters: dict[str, list[dict]],
        timeline: dict[str, Any],
        date_str: str,
        cluster_count: int,
    ) -> str:
        """Build v3.0 prompt with clustered data."""

        def safe_get_items(data: Any, key: str, default: list | None = None) -> list:
            """Safely extract items from raw_results, handling dict and list formats."""
            if default is None:
                default = []
            if not isinstance(data, dict):
                return default
            value = data.get(key, default)
            if not isinstance(value, list):
                return default
            return value

        # Organize data by source group - handle both dict and list formats
        hn_data = raw_results.get("hn", {})
        gh_data = raw_results.get("gh", {})

        ai_tech = {
            "hn": safe_get_items(hn_data, "stories", [])[:5] if isinstance(hn_data, dict) else (hn_data[:5] if isinstance(hn_data, list) else []),
            "github": safe_get_items(gh_data, "trending", [])[:5] if isinstance(gh_data, dict) else (gh_data[:5] if isinstance(gh_data, list) else []),
            "hf": raw_results.get("hf", [])[:5],
            "ai_news": raw_results.get("ai_news", [])[:5],
            "reddit": raw_results.get("reddit", [])[:3],
            "twitter": raw_results.get("twitter", [])[:3],
            "arxiv": raw_results.get("arxiv", [])[:3],
        }

        global_news = {
            "reuters": raw_results.get("reuters", [])[:5],
            "ap_news": raw_results.get("ap_news", [])[:5],
            "bbc": raw_results.get("bbc", [])[:3],
        }

        china_tech = {
            "jinri_remai": raw_results.get("jinri_remai", [])[:5],
            "sina": raw_results.get("sina", [])[:3],
            "ifeng": raw_results.get("ifeng", [])[:3],
        }

        china_society = {
            "pengpai": raw_results.get("pengpai", [])[:5],
            "caixin": raw_results.get("caixin", [])[:3],
        }

        prompt = f"""# Cloud927 v3.0 Report Data - {date_str}

## ğŸ“Š æ•°æ®æ¦‚è§ˆ
- **èšç±»æ•°é‡**: {cluster_count}
- **å…¨çƒè¦é—»**: {sum(len(v) for v in global_news.values())} ç¯‡
- **AIç§‘æŠ€**: {sum(len(v) for v in ai_tech.values())} ç¯‡
- **ä¸­å›½AI**: {sum(len(v) for v in china_tech.values())} ç¯‡
- **ä¸­å›½ç¤¾ä¼š**: {sum(len(v) for v in china_society.values())} ç¯‡

---

## ğŸŒ å…¨çƒè¦é—»

"""

        # Add global news
        for source, items in global_news.items():
            if items:
                prompt += f"### {source.upper()}\n"
                for item in items[:3]:
                    title = item.get("title", "N/A")
                    url = item.get("url", "")
                    timestamp = item.get("timestamp", 0)
                    if isinstance(timestamp, (int, float)):
                        time_str = datetime.fromtimestamp(timestamp).strftime("%H:%M")
                    else:
                        time_str = timestamp
                    prompt += f"- [{time_str}] [{title}]({url})\n"
                prompt += "\n"

        prompt += """## ğŸš€ AI ç§‘æŠ€å‰æ²¿

### èšç±»åˆ†æç»“æœ
"""
        # Add clustered data
        priority_categories = ["AIæ¨¡å‹æ›´æ–°", "å¼€æºå·¥å…·/æ¡†æ¶", "ç ”ç©¶è®ºæ–‡/çªç ´", "äº§å“å‘å¸ƒ/èèµ„"]
        for cat in priority_categories:
            if cat in clusters:
                items = clusters[cat][:3]
                prompt += f"\n**{cat}**\n"
                for item in items:
                    title = item.get("title", "N/A")[:60]
                    url = item.get("url", "")
                    source = item.get("source", "")
                    prompt += f"- [{source}] [{title}...]({url})\n"

        prompt += """
### ç¤¾åŒºçƒ­ç‚¹
"""
        # Add social/community data
        ai_tech_sources = [
            ("Hacker News", ai_tech.get("hn", [])),
            ("GitHub", ai_tech.get("github", [])),
            ("Reddit", ai_tech.get("reddit", [])),
            ("Twitter/X", ai_tech.get("twitter", [])),
        ]

        for source_name, items in ai_tech_sources:
            if items:
                prompt += f"**{source_name}**\n"
                for item in items[:2]:
                    title = item.get("title", "N/A")[:50]
                    url = item.get("url", "")
                    prompt += f"- [{title}]({url})\n"

        prompt += """
## ğŸ‡¨ğŸ‡³ ä¸­å›½åŠ¨æ€

### AI å‰æ²¿
"""
        for source, items in china_tech.items():
            if items:
                prompt += f"**{source.upper()}**\n"
                for item in items[:3]:
                    title = item.get("title", "N/A")[:50]
                    url = item.get("url", "")
                    prompt += f"- {title}\n"
                    if url:
                        prompt += f"  [é“¾æ¥]({url})\n"

        prompt += """
### ç¤¾ä¼šé‡å¤§
"""
        for source, items in china_society.items():
            if items:
                prompt += f"**{source.upper()}**\n"
                for item in items[:3]:
                    title = item.get("title", "N/A")[:50]
                    url = item.get("url", "")
                    prompt += f"- {title}\n"
                    if url:
                        prompt += f"  [é“¾æ¥]({url})\n"

        # Add timeline
        prompt += """
---

## ğŸ”® è¶‹åŠ¿è¿½è¸ª

"""
        new_entities = timeline.get("new", [])[:5]
        updated_entities = timeline.get("updated", [])[:5]

        if new_entities:
            prompt += "**æ–°å‡ºç°çš„å®ä½“:**\n"
            for item in new_entities:
                entity = item.get("entity", "Unknown")
                prompt += f"- {entity}\n"

        if updated_entities:
            prompt += "\n**æœ‰æ›´æ–°çš„å®ä½“:**\n"
            for item in updated_entities:
                entity = item.get("entity", "Unknown")
                prompt += f"- {entity}\n"

        prompt += """

---

## ğŸ“ ç”Ÿæˆè¦æ±‚

ä½¿ç”¨ v3.0 æ ¼å¼ç”Ÿæˆæ—¥æŠ¥:

1. **æ ¸å¿ƒå‘ç°** - ä¸€å¥è¯æ€»ç»“æœ€é‡è¦çš„ä¿¡å·
2. **å…¨çƒè¦é—»** - è¡¨æ ¼å±•ç¤ºï¼ŒæŒ‰æ—¶é—´æ’åº
3. **AIç§‘æŠ€** - ä½¿ç”¨èšç±»ç»“æœï¼ŒæŒ‰é‡è¦æ€§ç»„ç»‡
4. **ä¸­å›½åŠ¨æ€** - åˆ†ä¸ºAIå‰æ²¿å’Œç¤¾ä¼šé‡å¤§ä¸¤éƒ¨åˆ†
5. **è¶‹åŠ¿è¿½è¸ª** - å±•ç¤ºå®ä½“çš„æ¼”å˜
6. **æ´å¯Ÿ** - 100+å­—è¶‹åŠ¿åˆ¤æ–­ + å¯è¡ŒåŠ¨å»ºè®®

è¯·ä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œè¡¨æ ¼ç”¨äºå¯æ‰«ææ€§ï¼Œæ•£æ–‡ç”¨äºæ´å¯Ÿåˆ†æã€‚
"""

        return prompt
